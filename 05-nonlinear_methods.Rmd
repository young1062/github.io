# Manifold Learning {#ch-nonlinear}

In the previous sections, we have focused on methods which seek to approximate our data through a linear combination of feature vectors.  As we have seen, the resulting approximations live on linear (or affine) subspaces in the case of PCA and SVD and positive spans or convex combinations in the case of NMF.  While our data may exhibit some low-dimensional structure, there is no practical reason to expect such behavior to be inherently linear.   In the resulting sections, we will explore methods which consider **nonlinear** structure and assume the data reside on or near a manifold.  Such methods are referred to as nonlinear dimension reduction or manifold learning.  Critical to this discussion is the notion of a manifold.

::: {.definition #def-manifold name="Informal Definition of a Manifold"}
A manifold is a (topological) space which locally resembles Euclidean space.  Each point on a $k$-dimensional manifold has a neighborhood that can be mapped continuously to $\mathbb{R}^k$.
:::

To guide your intuition, think of a manifold as a smooth, possibly curved surface. Here are a few examples.

::: {.example #ex-manifolds name="Examples of Manifolds"}
Add line, sphere, plane, and S
:::

And here is an example of something which isn't a manifold.

::: {.example #ex-non-manifold name="Non-manifold"}
figure 8
:::

Much more could be said about the mathematical foundations of manifolds which are far beyond the scope of this book. For those interested in the such details consider checking out REFERENCES HERE.  We will appeal to a more intuitive understanding of manifolds and when necessary provide informal, descriptive "definitions" of important concepts.  For now, let's turn to the standard manifold assumption which is common to this area of unsupervised learning. 

<!-- Often, more restrictive assumptions are made about the manifold. -->

<!-- ::: {.example #def-diff-manifold name="Differentiable Manifolds"} -->
<!-- A differentiable manifold is a manifold which is sufficiently smooth so that we could do calculus on it. -->
<!-- ::: -->

<!-- ::: {.example #def-riem-manifold name="Riemannian Manifold"} -->
<!-- A Riemannian manifold is a smooth manifold with a notion of inner products. From the inner product, we can define a notion of angles and distances allowing one to define distances between point son the manifold. -->
<!-- ::: -->


## Data on a manifold 

In the simplest setting, we will assume there are points $\vec{z}_1,\dots,\vec{z}_N\in A \subset \mathbb{R}^k$ which are *iid* random samples. These points are (nonlinearly) mapped into a higher dimensional space $\mathbb{R}^d$ by a smooth map $\Psi$ giving data $\vec{x}_i = \Psi(\vec{z}_i)$ for $i=1,\dots,N.$   Hereafter, we refer to $\Psi$ as the manifold map.  In this setting, we are only given $\vec{x}_1,\dots,\vec{x}_N$, and we want to recover the lower-dimensional $\vec{z}_1,\dots,\vec{z}_N$. If possible, we would also like recover $\Psi$ and $\Psi^{-1}$ and in the most ideal case, the sampling distribution that generated the lower-dimensional coordinates $\vec{z}_1,\dots,\vec{z}_N$. 

::: {.example #ex-swiss-roll name="Mapping to the Swiss Roll"}
Let $A = (\pi/2,9\pi/2)\times (0,15)$.  We define the map $\Psi:A\to \mathbb{R}^3$ as follows
$$\Psi(\vec{z}) = \Psi(z_1,z_2) = \begin{bmatrix} z_1\sin(z_1) \\ z_1\cos(z_1) \\ z_2 \end{bmatrix}$$
Below we show $N=10^4$ \emph{iid} samples which are drawn uniformly from $A$.  We then show the resulting observations after applying map $\Psi$ to each sample.
```{r, echo = FALSE}
N <- 1e4
myColorRamp <- function(colors, values) {
    v <- (values - min(values))/diff(range(values))
    x <- colorRamp(colors)(v)
    rgb(x[,1], x[,2], x[,3], maxColorValue = 255)
}
library(scatterplot3d)
S <- matrix(NA, nrow = N, ncol = 2)
Swiss <- matrix(NA, nrow = N, ncol = 3)
for( n in 1:N){
    s <- runif(1, min = pi/2, max = 9*pi/2)
    t <- runif(1, min = 0,      max = 15)
    S[n,] <- c(s,t)
    Swiss[n, ] <- c( s*sin(s), s*cos(s),t )
}
par(mfrow = c(1,2))
plot(S[,1],S[,2],
     xlab = expression(z[1]), ylab = expression(z[2]),
     main = "Low-Dimensional Samples",
     cex = 0.1)
scatterplot3d(Swiss, color = myColorRamp(c("red","purple","blue","green","yellow"), S[,1] ),
               xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3]),
              angle = 90,
              main = "Samples after applying \nthe Manifold Map",
              cex.symbols = 0.5)
```
:::


We may also consider the more complicated case where the observations are corrupted by additive noise.  In this setting, the typical assumption is that the noise follows after the manifold map so that our data are $$\vec{x}_i = \Psi(\vec{z}_i) + \vec{\epsilon}_i, \qquad i = 1,\dots, N$$ for some \emph{iid} noise vectors $\{\vec{\epsilon}_i\}_{i=1,\dots,N}.$  

::: {.example #ex-swiss-w-noise name="Swiss Roll with Additive Gaussian Noise"}

Here, we perturb the observations in the preceding example with additive $\mathcal{N}(\vec{0},0.1{\bf I})$ noise.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library("threejs")
scatterplot3js(Swiss+matrix(rnorm(prod(dim(Swiss)),mean = 0,sd = 0.5), nrow = nrow(Swiss)), color = myColorRamp(c("red","purple","blue","green","yellow"), S[,1] ),
               xlab = expression(x[1]), ylab = expression(x[2]), zlab = expression(x[3]),
              angle = 90,
              main = "Swiss Roll Data Perturbed by Additive Noise", 
              pch = '.',
              size = 0.1)
```
:::

In addition to the goals in the noiseless case, we may also add the goal of learning the noiseless version of the data which reside on a manifold.

However, there are a number of practical issues to this setup.  First, the dimension, $k$, of the original lower-dimensional points is typically unknown.  Similar to previous methods, we could pick a value of $k$ with the goal of visualization, base our choice off of prior knowledge, or run our algorithms different choices of $k$ and compare the results.  More advanced methods for estimating the true value of $k$ are an open area of research (REFERENCES NEEDED).  

There is also a issue with the uniqueness problem statement.  Given only the high dimensional observations, there is no way we could identify the original lower-dimensional points without more information.  In fact, one could find an unlimited sources of equally suitable results.  Here is the issue.

Let $\Phi:\mathbb{R}^k\to\mathbb{R}^k$ be some invertible function.  As an example, you could think of $\Phi$ as defining a translation, reflection, rotation, or some composition of these operations.  If our original observed data are $\vec{x}_i = \Psi(\vec{z}_i)$, our manifold learning algorithm could instead infer that the manifold map is $\Psi \circ \Phi^{-1}$ and the lower-dimensional points are $\Phi(\vec{z}_i)$. This is a perfectly reasonable result since $(\Psi\circ \Phi^{-1}\circ)\Phi(\vec{z}_i) = \Psi(\vec{z}_i)= \vec{x}_i$ for $i=1,\dots,N$, which is the only result we require.  Without additional information, there is little we could do to address this issue.  For the purposes of visualization, however, we will typically be most interested in the relationship between the lower-dimensional points rather than their specific location or orientation.  As such, we need not be concerned about a manifold learning algorithm that provides a translated or rotated representation of $\vec{z}_1,\dots,\vec{z}_N.$ More complicated transformations of the lower-dimensional coordinates are of greater concern and may be addressed through additional assumptions about the manifold map $\Psi.$

In the following sections, we will review a small collection of different methods which address the manifold learning problem.  This collection is by no means exhaustive so we provide a small list with associated references to conclude the chapter.


<!-- ISOMAP -->
```{r child = 'topics/ISOMAP.Rmd'}
```

<!-- LLE -->
```{r child = 'topics/LLE.Rmd'}
```

<!-- Laplacian Eigenmaps -->
```{r child = 'topics/laplacian_eigenmaps.Rmd'}
```

<!-- AE -->
```{r child = 'topics/AE.Rmd'}
```

## Additional methods

## Exercises




