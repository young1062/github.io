# Clustering {#ch-clustering}

We now turn to clustering, which is the branch of UL focused on partitioning our data into subgroups of similar observations.  Hereafter, we use the terms clusters and subgroups interchangeably.  There are naturally a number of questions which arise including:

i) how many clusters (if any) exist?
ii) what do we mean by similar observations?

As we will see through a series of examples, these two points are linked. Given a notion of similarity (which are implicitly defined for different clustering algorithms and tuneable for others), a certain number of clusters and clustering of data may be optimal. Given a different notion of similarity, an entirely different organization of the data into a different number of clusters may arise. 

For now, let us focus on the the latter question regarding similarity.  Consider the following three examples of data sets in $\mathbb{R}^2$.  Visually, how would you cluster the observations? In particular, how many subgroups would you say exist and how would your partition the data into these subgroups?

::: {.example #cluster-what name="Different notions of similar subgroups"}
```{r fig-cluster-what, fig.align='center', fig.width=10, echo = FALSE}
set.seed(185)
library("MASS")
N <- 200
rho <- 3/4
data1 <- matrix(rnorm(N*2, mean = -2), nrow = N) + matrix((runif(N) < 0.5),nrow = N) %*% matrix(c(4,4),nrow = 1)
data2 <- rbind(mvrnorm(N/2, mu = c(-2,-2), Sigma = matrix(c(1,rho,rho,1),nrow = 2)),
               mvrnorm(N/2, mu = c(2,2), Sigma = matrix(c(1,-rho,-rho,1),nrow = 2)))
data3 <- matrix(rnorm(N*2), nrow = N)
data3 <- diag(1 + (runif(N) < 0.5) + (runif(N) < 0.5)) %*% diag(1/sqrt(diag(data3 %*% t(data3)))) %*% data3
par(mfrow = c(1,3))
plot(data1, xlab = '', ylab = '', main = "Case 1")
plot(data2, xlab = '', ylab = '', main = "Case 2")
plot(data3, xlab = '', ylab = '', main = "Case 3")
```
:::

The questions posed above can be reasonably answered based on the figures but will not be so easy in high dimensions.  We'll explore these questions and discuss data driven ways to estimate the appropriate number of cluster **for a specific algorithm** in the following sections.

There is one main commonality (and associated set of notation) that will persist throughout this section.  The ultimate goal of clustering is to separate data $\vec{x}_1,\dots,\vec{x}_N$ into $k$ groups. The choice of $k$ is critical in clustering.  Some algorithms proceed by iteratively merging observations until only $k$ clusters remain (hierarchical methods). Others (center- and model-based and spectral methods) treat clustering as a partitioning problem, where each element of the partition corresponds to a pre-specified number of clusters. Algorithm dependent methods for choosing $k$ can be used for deciding optimal number of clusters without a priori knowledge.


<!-- Center based clustering -->
```{r child = 'topics/Center-Based-Clustering.Rmd'}
```

<!-- Hierarchical -->  
```{r child = 'topics/Hierarchical-Clustering.Rmd'}
```

<!-- Model based -->
<!-- # ```{r child = 'topics/Model-Based-Clustering.Rmd'} -->
<!-- # ``` -->

<!-- Spectral Clustering -->

<!-- # ```{r child = 'topics/Spectral-Clustering.Rmd'} -->
<!-- # ``` -->

<!-- Clustering comparisons and metrics -->
