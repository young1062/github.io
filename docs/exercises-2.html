<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.5 Exercises | An Introduction to Unsupervised Learning</title>
  <meta name="description" content="An introductory text on the goals and methods of unsupervised learning" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="4.5 Exercises | An Introduction to Unsupervised Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.5 Exercises | An Introduction to Unsupervised Learning" />
  
  <meta name="twitter:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

<meta name="author" content="Alex Young and Cenhao Zhu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec-mds.html"/>
<link rel="next" href="kernels-and-nonlinearity.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/threejs-111/three.min.js"></script>
<script src="libs/threejs-111/Detector.js"></script>
<script src="libs/threejs-111/Projector.js"></script>
<script src="libs/threejs-111/CanvasRenderer.js"></script>
<script src="libs/threejs-111/TrackballControls.js"></script>
<script src="libs/threejs-111/StateOrbitControls.js"></script>
<script src="libs/scatterplotThree-binding-0.3.3/scatterplotThree.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Unsupervised Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-prob.html"><a href="ch-prob.html"><i class="fa fa-check"></i><b>2</b> Mathematical Background and Notation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="important-notation.html"><a href="important-notation.html"><i class="fa fa-check"></i><b>2.1</b> Important notation</a></li>
<li class="chapter" data-level="2.2" data-path="random-vectors-in-mathbbrd.html"><a href="random-vectors-in-mathbbrd.html"><i class="fa fa-check"></i><b>2.2</b> Random vectors in <span class="math inline">\(\mathbb{R}^d\)</span></a></li>
<li class="chapter" data-level="2.3" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html"><i class="fa fa-check"></i><b>2.3</b> Expectation, Mean, and Covariance</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#sample-mean-and-sample-covariance"><i class="fa fa-check"></i><b>2.3.1</b> Sample Mean and Sample Covariance</a></li>
<li class="chapter" data-level="2.3.2" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#the-data-matrix"><i class="fa fa-check"></i><b>2.3.2</b> The Data Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>2.4</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="linear-algebra.html"><a href="linear-algebra.html#assumed-background"><i class="fa fa-check"></i><b>2.4.1</b> Assumed Background</a></li>
<li class="chapter" data-level="2.4.2" data-path="linear-algebra.html"><a href="linear-algebra.html#interpretations-of-matrix-multiplication"><i class="fa fa-check"></i><b>2.4.2</b> Interpretations of Matrix Multiplication</a></li>
<li class="chapter" data-level="2.4.3" data-path="linear-algebra.html"><a href="linear-algebra.html#norms-and-distances"><i class="fa fa-check"></i><b>2.4.3</b> Norms and Distances</a></li>
<li class="chapter" data-level="2.4.4" data-path="linear-algebra.html"><a href="linear-algebra.html#important-properties"><i class="fa fa-check"></i><b>2.4.4</b> Important properties</a></li>
<li class="chapter" data-level="2.4.5" data-path="linear-algebra.html"><a href="linear-algebra.html#matrix-factorizations"><i class="fa fa-check"></i><b>2.4.5</b> Matrix Factorizations</a></li>
<li class="chapter" data-level="2.4.6" data-path="linear-algebra.html"><a href="linear-algebra.html#positive-definiteness-and-matrix-powers"><i class="fa fa-check"></i><b>2.4.6</b> Positive Definiteness and Matrix Powers</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="exercises.html"><a href="exercises.html#probability"><i class="fa fa-check"></i><b>2.5.1</b> Probability</a></li>
<li class="chapter" data-level="2.5.2" data-path="exercises.html"><a href="exercises.html#calculus"><i class="fa fa-check"></i><b>2.5.2</b> Calculus</a></li>
<li class="chapter" data-level="2.5.3" data-path="exercises.html"><a href="exercises.html#linear-algebra-1"><i class="fa fa-check"></i><b>2.5.3</b> Linear Algebra</a></li>
<li class="chapter" data-level="2.5.4" data-path="exercises.html"><a href="exercises.html#hybrid-problems"><i class="fa fa-check"></i><b>2.5.4</b> Hybrid Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html"><i class="fa fa-check"></i><b>3</b> Central goals and assumptions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="dimension-reduction-and-manifold-learning.html"><a href="dimension-reduction-and-manifold-learning.html"><i class="fa fa-check"></i><b>3.1</b> Dimension reduction and manifold learning</a></li>
<li class="chapter" data-level="3.2" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>3.2</b> Clustering</a></li>
<li class="chapter" data-level="3.3" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html"><i class="fa fa-check"></i><b>3.3</b> Generating synthetic data</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html#data-on-manifolds"><i class="fa fa-check"></i><b>3.3.1</b> Data on manifolds</a></li>
<li class="chapter" data-level="3.3.2" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html#clustered-data"><i class="fa fa-check"></i><b>3.3.2</b> Clustered data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-linear.html"><a href="ch-linear.html"><i class="fa fa-check"></i><b>4</b> Linear Methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec-pca.html"><a href="sec-pca.html"><i class="fa fa-check"></i><b>4.1</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pca.html"><a href="sec-pca.html#derivation-using-iterative-projections"><i class="fa fa-check"></i><b>4.1.1</b> Derivation using Iterative Projections</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-svd.html"><a href="sec-svd.html"><i class="fa fa-check"></i><b>4.2</b> Singular Value Decomposition</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="sec-svd.html"><a href="sec-svd.html#low-rank-approximations"><i class="fa fa-check"></i><b>4.2.1</b> Low-rank approximations</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-svd.html"><a href="sec-svd.html#svd-and-low-rank-approximations"><i class="fa fa-check"></i><b>4.2.2</b> SVD and Low Rank Approximations</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-svd.html"><a href="sec-svd.html#connections-with-pca"><i class="fa fa-check"></i><b>4.2.3</b> Connections with PCA</a></li>
<li class="chapter" data-level="4.2.4" data-path="sec-svd.html"><a href="sec-svd.html#recommender-systems"><i class="fa fa-check"></i><b>4.2.4</b> Recommender Systems</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html"><i class="fa fa-check"></i><b>4.3</b> Nonnegative Matrix Factorization</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#interpretability-superpositions-and-positive-spans"><i class="fa fa-check"></i><b>4.3.1</b> Interpretability, Superpositions, and Positive Spans</a></li>
<li class="chapter" data-level="4.3.2" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#geometric-interpretation"><i class="fa fa-check"></i><b>4.3.2</b> Geometric Interpretation</a></li>
<li class="chapter" data-level="4.3.3" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#finding-an-nmf-multiplicative-updates"><i class="fa fa-check"></i><b>4.3.3</b> Finding an NMF: Multiplicative Updates</a></li>
<li class="chapter" data-level="4.3.4" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#nmf-in-practice"><i class="fa fa-check"></i><b>4.3.4</b> NMF in practice</a></li>
<li class="chapter" data-level="4.3.5" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#sec-nmf-ext"><i class="fa fa-check"></i><b>4.3.5</b> Regularization and Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="sec-mds.html"><a href="sec-mds.html"><i class="fa fa-check"></i><b>4.4</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec-mds.html"><a href="sec-mds.html#key-features-of-mds"><i class="fa fa-check"></i><b>4.4.1</b> Key features of MDS</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec-mds.html"><a href="sec-mds.html#classical-scaling"><i class="fa fa-check"></i><b>4.4.2</b> Classical Scaling</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec-mds.html"><a href="sec-mds.html#metric-mds"><i class="fa fa-check"></i><b>4.4.3</b> Metric MDS</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec-mds.html"><a href="sec-mds.html#nonmetric-mds"><i class="fa fa-check"></i><b>4.4.4</b> Nonmetric MDS</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="kernels-and-nonlinearity.html"><a href="kernels-and-nonlinearity.html"><i class="fa fa-check"></i><b>5</b> Kernels and Nonlinearity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="kernel-pca.html"><a href="kernel-pca.html"><i class="fa fa-check"></i><b>5.1</b> Kernel PCA</a></li>
<li class="chapter" data-level="5.2" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Manifold Learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data-on-a-manifold.html"><a href="data-on-a-manifold.html"><i class="fa fa-check"></i><b>6.1</b> Data on a manifold</a></li>
<li class="chapter" data-level="6.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html"><i class="fa fa-check"></i><b>6.2</b> Isometric Feature Map (ISOMAP)</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#introduction"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#key-definitions"><i class="fa fa-check"></i><b>6.2.2</b> Key Definitions</a></li>
<li class="chapter" data-level="6.2.3" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#algorithm"><i class="fa fa-check"></i><b>6.2.3</b> Algorithm</a></li>
<li class="chapter" data-level="6.2.4" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#limitations-of-isomap"><i class="fa fa-check"></i><b>6.2.4</b> Limitations of ISOMAP</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html"><i class="fa fa-check"></i><b>6.3</b> Locally Linear Embeddings (LLEs)</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#introduction-1"><i class="fa fa-check"></i><b>6.3.1</b> Introduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#algorithm-1"><i class="fa fa-check"></i><b>6.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.3.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#strengths-and-weaknesses-of-lle"><i class="fa fa-check"></i><b>6.3.3</b> Strengths and Weaknesses of LLE</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="laplacian-eigenmap.html"><a href="laplacian-eigenmap.html"><i class="fa fa-check"></i><b>6.4</b> Laplacian Eigenmap</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="laplacian-eigenmap.html"><a href="laplacian-eigenmap.html#algorithm-2"><i class="fa fa-check"></i><b>6.4.1</b> Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html"><i class="fa fa-check"></i><b>6.5</b> Autoencoders (AEs)</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#introduction-2"><i class="fa fa-check"></i><b>6.5.1</b> Introduction</a></li>
<li class="chapter" data-level="6.5.2" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#algorithm-3"><i class="fa fa-check"></i><b>6.5.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.5.3" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#example"><i class="fa fa-check"></i><b>6.5.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="additional-methods.html"><a href="additional-methods.html"><i class="fa fa-check"></i><b>6.6</b> Additional methods</a></li>
<li class="chapter" data-level="6.7" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-clustering.html"><a href="ch-clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="center-based.html"><a href="center-based.html"><i class="fa fa-check"></i><b>7.1</b> Center-based</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="center-based.html"><a href="center-based.html#k-means"><i class="fa fa-check"></i><b>7.1.1</b> k-means</a></li>
<li class="chapter" data-level="7.1.2" data-path="center-based.html"><a href="center-based.html#k-mediods"><i class="fa fa-check"></i><b>7.1.2</b> k-mediods</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="hierarchical.html"><a href="hierarchical.html"><i class="fa fa-check"></i><b>7.2</b> Hierarchical</a></li>
<li class="chapter" data-level="7.3" data-path="model-based.html"><a href="model-based.html"><i class="fa fa-check"></i><b>7.3</b> Model-based</a></li>
<li class="chapter" data-level="7.4" data-path="spectral.html"><a href="spectral.html"><i class="fa fa-check"></i><b>7.4</b> Spectral</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Unsupervised Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="exercises-2" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Exercises<a href="exercises-2.html#exercises-2" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<ol style="list-style-type: decimal">
<li><p>Show that the PCA scores are centered.</p></li>
<li><p>Suppose a data matrix <span class="math inline">\({\bf X}\in\mathbb{R}^{N\times d}\)</span> has principal component scores <span class="math inline">\({\bf Y}\in\mathbb{R}^{N\times d}\)</span>, principal component loading matrix <span class="math inline">\({\bf W}\in\mathbb{R}^{d\times d}\)</span>, and principal component variances <span class="math inline">\(\lambda_1,\dots,\lambda_d\)</span>. Show that the sample covariance of the PCA scores, <span class="math inline">\({\bf Y}\)</span>, is diagonal.</p></li>
<li><p>Consider the data matrix <span class="math display">\[{\bf X} = \begin{bmatrix} 2 &amp; 3 \\ 4 &amp; 5\\ 6&amp; 7 \\ 8 &amp; 9 \end{bmatrix}.\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Compute the principal component scores, variances, and loadings of <span class="math inline">\({\bf X}.\)</span></li>
<li>Does <span class="math inline">\({\bf X}\)</span> exhibit lower dimensional structure? If so, describe it.</li>
</ol></li>
<li><p>Given a dataset with outliers:
<span class="math display">\[
\mathbf{X} = \begin{bmatrix}
1 &amp; 2 \\
2 &amp; 4 \\
3 &amp; 6 \\
4 &amp; 8 \\
100 &amp; 200
\end{bmatrix}
\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Center the data.</li>
<li>Compute the covariance matrix.</li>
<li>Perform PCA and identify the principal component scores, loadings, and variances.</li>
<li>Discuss how outliers affect PCA and suggest ways to handle them.</li>
</ol></li>
<li><p>Consider a dataset:
<span class="math display">\[
\mathbf{X} = \begin{bmatrix}
1 &amp; 100 \\
2 &amp; 200 \\
3 &amp; 300 \\
4 &amp; 400
\end{bmatrix}
\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Center the data.</li>
<li>Compute the covariance matrix.</li>
<li>Perform PCA and find the principal components.</li>
<li>Discuss the importance of feature scaling in PCA.</li>
</ol></li>
<li><p>Given a nonlinear dataset:
<span class="math display">\[\mathbf{X} = \begin{bmatrix}
1 &amp; 1 \\
2 &amp; 4 \\
3 &amp; 9 \\
4 &amp; 16 \\
5 &amp; 25
\end{bmatrix}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Center the data.</li>
<li>Compute the covariance matrix.</li>
<li>Perform PCA and identify the principal components.</li>
<li>Discuss the limitations of PCA for nonlinear datasets and suggest alternative methods.</li>
</ol></li>
<li><p>Load the Iris dataset and standardize the features.</p>
<ol style="list-style-type: lower-alpha">
<li>Perform PCA on the standardized data.</li>
<li>Plot the cumulative explained variance as a function of the number of principal components.</li>
<li>Choose the number of principal components that explain at least 95% of the variance.</li>
<li>Project the data onto the chosen principal components and visualize the results.</li>
<li>Discuss the results and the effectiveness of PCA in reducing the dimensionality of the dataset.</li>
</ol></li>
<li><p>Suppose <span class="math display">\[\vec{x}=(x_1,x_2)^T \sim \mathcal{N}\left(\begin{bmatrix} 0\\ 0\end{bmatrix},
\begin{bmatrix}1 &amp; \rho \\ \rho &amp; 1\end{bmatrix}\right),\]</span> i.e. random vector <span class="math inline">\(\vec{x}\)</span> follows a multivariate normal distribution with mean <span class="math inline">\(\vec{0}\)</span> and covariance <span class="math inline">\(\Sigma_X = \begin{bmatrix}1 &amp; \rho \\ \rho &amp; 1\end{bmatrix}\)</span>. Let <span class="math inline">\(w = \frac{\sqrt{2}}{2}(x_1+x_2)\)</span> and <span class="math inline">\(z = \frac{\sqrt{2}}{2}(x_1-x_2).\)</span> Find the joint distribution of <span class="math inline">\((w,z)^T\)</span>. Hint: linear combinations of normal random variables are also normal.</p></li>
<li><p>Consider a data matrix <span class="math inline">\(X \in \mathbb{R}^{N\times d}\)</span> with centered columns so that the sample covariance matrix is
<span class="math display">\[\hat{\Sigma} = \frac{X^TX}{N}.\]</span>
Assume <span class="math inline">\(\hat{\Sigma}\)</span> has eigenvalues <span class="math inline">\(\lambda_1 &gt; \lambda_2 &gt;\dots &gt; \lambda_d &gt;0\)</span> with orthonormal eigenvectors <span class="math inline">\(\vec{w_1},\dots,\vec{w}_d.\)</span></p>
<ol style="list-style-type: lower-alpha">
<li><p>If <span class="math inline">\(X^{(1)} = X - X\vec{w}_1\vec{w}_1^T\)</span> is the data matrix where each row has had its component in the direction of <span class="math inline">\(\vec{w}_1\)</span> removed, show that <span class="math display">\[\hat{\Sigma}^{(1)} = \frac{{X^{(1)}}^T X^{(1)}}{N} = \hat{\Sigma} - \lambda_1 \vec{w}_1\vec{w}_1^T.\]</span></p></li>
<li><p>Show that <span class="math inline">\(\hat{\Sigma}\)</span> can be written in the form <span class="math inline">\(\hat{\Sigma} = \sum_{j=1}^d \lambda_j \vec{w}_j\vec{w}_j^T.\)</span></p></li>
</ol></li>
<li><p>For <span class="math inline">\(k &lt; d\)</span>, let <span class="math inline">\(\vec{q}_1,\dots,\vec{q}_k\in\mathbb{R}^d\)</span> be fixed orthonormal vectors. Suppose <span class="math inline">\(a_1,\dots,a_k\)</span> are independent Gaussian random variables with mean zero and variances <span class="math inline">\(\lambda_1&gt;\dots&gt;\lambda_k\)</span> respectively. Let <span class="math display">\[\vec{x} = a_1\vec{q}_1+\dots+a_k+\vec{q}_k + \vec{\epsilon}\]</span> where <span class="math inline">\(\vec{\epsilon}\sim\mathcal{N}(\vec{0},\sigma^2{\bf I})\)</span> is independent of <span class="math inline">\(a_1,\dots,a_k\)</span></p>
<ol style="list-style-type: lower-alpha">
<li>Find the mean and covariance of <span class="math inline">\({\bf X}\)</span>.</li>
<li>Find the eigenvalues and eigenvectors of <span class="math inline">\({\bf \Sigma}\)</span>.<br />
</li>
<li>For <span class="math inline">\(d = 10\)</span> and $k=3 and let <span class="math inline">\(\lambda_1 = 25,\, \lambda_2 = 9,\,\lambda_3 = 4\)</span> and <span class="math inline">\(\sigma^2=1\)</span>. Generate 100 independent realizations of <span class="math inline">\(\vec{x}\)</span> and compute the principal component loadings and variances. How do these compare to your results from b.</li>
<li>Repeat c. for <span class="math inline">\(10^4\)</span> samples. How have the results changed?</li>
</ol></li>
<li><p>Let <span class="math inline">\(\vec{x}_1,\dots,\vec{x}_N\)</span> be vectors in <span class="math inline">\(\mathbb{R}^d\)</span>. Assume a PCA of these data has loadings <span class="math inline">\(\vec{w}_1,\dots,\vec{w}_d\)</span> with associated variances <span class="math inline">\(\lambda_1 \ge \dots \ge \lambda_d \ge 0.\)</span> Let <span class="math inline">\(U\)</span> be a <span class="math inline">\(d\times d\)</span> orthonormal matrix and set <span class="math inline">\(\vec{y}_i = U\vec{x}_i.\)</span> Find expressions for the principal component loadings and variance of <span class="math inline">\(\vec{y}_1,\dots,\vec{y}_N\)</span> in terms of <span class="math inline">\(U\)</span>, <span class="math inline">\(\vec{w}_1,\dots,\vec{w}_d\)</span> and <span class="math inline">\(\lambda_1,\dots,\lambda_d.\)</span></p></li>
<li><p>Load the <em>mtcar</em> dataset containing 11 observations from 32 cars.</p>
<ol style="list-style-type: lower-alpha">
<li><p>Show the principal component loadings and a biplot of the first and second PC components.</p></li>
<li><p>Rescale the mpg (miles per gallon) data to feet per gallon, i.e. <em>mtcars$mpg &lt;- 5280</em>mtcars$mpg.* Rerun PCA on these modified data and show the loadings and biplot.</p></li>
<li><p>What is the first loading capturing? Explain this result.</p></li>
<li><p>Rerun the results using the empirical correlation matrix by setting the option <em>scale = TRUE</em> in the prcomp command. Compare this result with part (a)</p></li>
</ol></li>
<li><p>Consider the helix data shown below from two different directions.</p></li>
</ol>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-14"></span>
<img src="_main_files/figure-html/unnamed-chunk-14-1.png" alt="Helix data" width="672" />
<p class="caption">
Figure 4.6: Helix data
</p>
</div>
<ol style="list-style-type: lower-alpha">
<li><p>What is the dimension of the shape formed by these data?</p></li>
<li><p>Compute the three principal component variances and show them below.</p></li>
<li><p>Do the principal component variances reflect the dimension of the data? Why or why not?</p></li>
</ol>
<ol style="list-style-type: decimal">
<li>Consider a dataset with three features:
<span class="math display">\[ \mathbf{X} = \begin{bmatrix}
2 &amp; 0 &amp; 1 \\
3 &amp; 2 &amp; 2 \\
4 &amp; 4 &amp; 3 \\
5 &amp; 6 &amp; 4 \\
6 &amp; 8 &amp; 5
\end{bmatrix}\]</span></li>
</ol>
<ol style="list-style-type: lower-alpha">
<li>Center the data by subtracting the mean of each feature.</li>
<li>Compute the covariance matrix of the centered data.</li>
<li>Find the eigenvalues and eigenvectors of the covariance matrix.</li>
<li>Project the data onto the first two principal components.</li>
<li>Visualize the original data and the projected data in a 2D plot.</li>
<li>Discuss how PCA can help in reducing the dimensionality of the data while preserving important information.</li>
</ol>
<ol style="list-style-type: decimal">
<li><p>The dataset <em>simplex10</em> contains samples generated from the probability simplex in <span class="math inline">\(\mathbb{R}^{10}\)</span>. (This means all entries nonnegative and each random vector has entries which sum to one.)</p>
<ol style="list-style-type: lower-alpha">
<li><p>Run PCA, SVD, and NMF on these data and compare the results. In particular, what lower dimensional structure, if any, do these methods indicate?</p></li>
<li><p>Why do the results differ?</p></li>
</ol></li>
<li><p>Show that the Frobenius norm of a matrix is the square of the sum of its squared singular values. You may use the helpful identity <span class="math inline">\(\|{\bf A}\|_F^2 = tr({\bf AA}^T)}\)</span></p></li>
</ol>
<p>Given a non-negative matrix:
<span class="math display">\[\mathbf{X} = \begin{bmatrix}
5 &amp; 4 &amp; 2 \\
4 &amp; 5 &amp; 3 \\
2 &amp; 3 &amp; 5
\end{bmatrix}\]</span></p>
<ol style="list-style-type: lower-alpha">
<li>Factorize <span class="math inline">\(\mathbf{V}\)</span> with different ranks (e.g., <span class="math inline">\(r = 1\)</span>, <span class="math inline">\(r = 2\)</span>, and <span class="math inline">\(r = 3\)</span>).</li>
<li>Discuss how to choose the appropriate rank for NMF in practice.</li>
</ol>

</div>
<!-- </div> -->
            </section>

          </div>
        </div>
      </div>
<a href="sec-mds.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="kernels-and-nonlinearity.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/young1062/introUL03-linear_methods.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
