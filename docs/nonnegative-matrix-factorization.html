<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.3 Nonnegative Matrix Factorization | An Introduction to Unsupervised Learning</title>
  <meta name="description" content="An introductory text on the goals and methods of unsupervised learning" />
  <meta name="generator" content="bookdown 0.40 and GitBook 2.6.7" />

  <meta property="og:title" content="4.3 Nonnegative Matrix Factorization | An Introduction to Unsupervised Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.3 Nonnegative Matrix Factorization | An Introduction to Unsupervised Learning" />
  
  <meta name="twitter:description" content="An introductory text on the goals and methods of unsupervised learning" />
  

<meta name="author" content="Alex Young and Cenhao Zhu" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="sec-svd.html"/>
<link rel="next" href="sec-mds.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<link href="libs/htmltools-fill-0.5.8.1/fill.css" rel="stylesheet" />
<script src="libs/htmlwidgets-1.6.4/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.1/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/threejs-111/three.min.js"></script>
<script src="libs/threejs-111/Detector.js"></script>
<script src="libs/threejs-111/Projector.js"></script>
<script src="libs/threejs-111/CanvasRenderer.js"></script>
<script src="libs/threejs-111/TrackballControls.js"></script>
<script src="libs/threejs-111/StateOrbitControls.js"></script>
<script src="libs/scatterplotThree-binding-0.3.3/scatterplotThree.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Unsupervised Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="prerequisites.html"><a href="prerequisites.html"><i class="fa fa-check"></i><b>1.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="ch-prob.html"><a href="ch-prob.html"><i class="fa fa-check"></i><b>2</b> Mathematical Background and Notation</a>
<ul>
<li class="chapter" data-level="2.1" data-path="important-notation.html"><a href="important-notation.html"><i class="fa fa-check"></i><b>2.1</b> Important notation</a></li>
<li class="chapter" data-level="2.2" data-path="random-vectors-in-mathbbrd.html"><a href="random-vectors-in-mathbbrd.html"><i class="fa fa-check"></i><b>2.2</b> Random vectors in <span class="math inline">\(\mathbb{R}^d\)</span></a></li>
<li class="chapter" data-level="2.3" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html"><i class="fa fa-check"></i><b>2.3</b> Expectation, Mean, and Covariance</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#sample-mean-and-sample-covariance"><i class="fa fa-check"></i><b>2.3.1</b> Sample Mean and Sample Covariance</a></li>
<li class="chapter" data-level="2.3.2" data-path="expectation-mean-and-covariance.html"><a href="expectation-mean-and-covariance.html#the-data-matrix"><i class="fa fa-check"></i><b>2.3.2</b> The Data Matrix</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="linear-algebra.html"><a href="linear-algebra.html"><i class="fa fa-check"></i><b>2.4</b> Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="linear-algebra.html"><a href="linear-algebra.html#assumed-background"><i class="fa fa-check"></i><b>2.4.1</b> Assumed Background</a></li>
<li class="chapter" data-level="2.4.2" data-path="linear-algebra.html"><a href="linear-algebra.html#interpretations-of-matrix-multiplication"><i class="fa fa-check"></i><b>2.4.2</b> Interpretations of Matrix Multiplication</a></li>
<li class="chapter" data-level="2.4.3" data-path="linear-algebra.html"><a href="linear-algebra.html#norms-and-distances"><i class="fa fa-check"></i><b>2.4.3</b> Norms and Distances</a></li>
<li class="chapter" data-level="2.4.4" data-path="linear-algebra.html"><a href="linear-algebra.html#important-properties"><i class="fa fa-check"></i><b>2.4.4</b> Important properties</a></li>
<li class="chapter" data-level="2.4.5" data-path="linear-algebra.html"><a href="linear-algebra.html#matrix-factorizations"><i class="fa fa-check"></i><b>2.4.5</b> Matrix Factorizations</a></li>
<li class="chapter" data-level="2.4.6" data-path="linear-algebra.html"><a href="linear-algebra.html#positive-definiteness-and-matrix-powers"><i class="fa fa-check"></i><b>2.4.6</b> Positive Definiteness and Matrix Powers</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="exercises.html"><a href="exercises.html"><i class="fa fa-check"></i><b>2.5</b> Exercises</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="exercises.html"><a href="exercises.html#probability"><i class="fa fa-check"></i><b>2.5.1</b> Probability</a></li>
<li class="chapter" data-level="2.5.2" data-path="exercises.html"><a href="exercises.html#calculus"><i class="fa fa-check"></i><b>2.5.2</b> Calculus</a></li>
<li class="chapter" data-level="2.5.3" data-path="exercises.html"><a href="exercises.html#linear-algebra-1"><i class="fa fa-check"></i><b>2.5.3</b> Linear Algebra</a></li>
<li class="chapter" data-level="2.5.4" data-path="exercises.html"><a href="exercises.html#hybrid-problems"><i class="fa fa-check"></i><b>2.5.4</b> Hybrid Problems</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="central-goals-and-assumptions.html"><a href="central-goals-and-assumptions.html"><i class="fa fa-check"></i><b>3</b> Central goals and assumptions</a>
<ul>
<li class="chapter" data-level="3.1" data-path="dimension-reduction-and-manifold-learning.html"><a href="dimension-reduction-and-manifold-learning.html"><i class="fa fa-check"></i><b>3.1</b> Dimension reduction and manifold learning</a></li>
<li class="chapter" data-level="3.2" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>3.2</b> Clustering</a></li>
<li class="chapter" data-level="3.3" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html"><i class="fa fa-check"></i><b>3.3</b> Generating synthetic data</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html#data-on-manifolds"><i class="fa fa-check"></i><b>3.3.1</b> Data on manifolds</a></li>
<li class="chapter" data-level="3.3.2" data-path="generating-synthetic-data.html"><a href="generating-synthetic-data.html#clustered-data"><i class="fa fa-check"></i><b>3.3.2</b> Clustered data</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="exercises-1.html"><a href="exercises-1.html"><i class="fa fa-check"></i><b>3.4</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ch-linear.html"><a href="ch-linear.html"><i class="fa fa-check"></i><b>4</b> Linear Methods</a>
<ul>
<li class="chapter" data-level="4.1" data-path="sec-pca.html"><a href="sec-pca.html"><i class="fa fa-check"></i><b>4.1</b> Principal Component Analysis</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="sec-pca.html"><a href="sec-pca.html#derivation-using-iterative-projections"><i class="fa fa-check"></i><b>4.1.1</b> Derivation using Iterative Projections</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="sec-svd.html"><a href="sec-svd.html"><i class="fa fa-check"></i><b>4.2</b> Singular Value Decomposition</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="sec-svd.html"><a href="sec-svd.html#low-rank-approximations"><i class="fa fa-check"></i><b>4.2.1</b> Low-rank approximations</a></li>
<li class="chapter" data-level="4.2.2" data-path="sec-svd.html"><a href="sec-svd.html#svd-and-low-rank-approximations"><i class="fa fa-check"></i><b>4.2.2</b> SVD and Low Rank Approximations</a></li>
<li class="chapter" data-level="4.2.3" data-path="sec-svd.html"><a href="sec-svd.html#connections-with-pca"><i class="fa fa-check"></i><b>4.2.3</b> Connections with PCA</a></li>
<li class="chapter" data-level="4.2.4" data-path="sec-svd.html"><a href="sec-svd.html#recommender-systems"><i class="fa fa-check"></i><b>4.2.4</b> Recommender Systems</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html"><i class="fa fa-check"></i><b>4.3</b> Nonnegative Matrix Factorization</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#interpretability-superpositions-and-positive-spans"><i class="fa fa-check"></i><b>4.3.1</b> Interpretability, Superpositions, and Positive Spans</a></li>
<li class="chapter" data-level="4.3.2" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#geometric-interpretation"><i class="fa fa-check"></i><b>4.3.2</b> Geometric Interpretation</a></li>
<li class="chapter" data-level="4.3.3" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#finding-an-nmf-multiplicative-updates"><i class="fa fa-check"></i><b>4.3.3</b> Finding an NMF: Multiplicative Updates</a></li>
<li class="chapter" data-level="4.3.4" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#nmf-in-practice"><i class="fa fa-check"></i><b>4.3.4</b> NMF in practice</a></li>
<li class="chapter" data-level="4.3.5" data-path="nonnegative-matrix-factorization.html"><a href="nonnegative-matrix-factorization.html#sec-nmf-ext"><i class="fa fa-check"></i><b>4.3.5</b> Regularization and Interpretability</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="sec-mds.html"><a href="sec-mds.html"><i class="fa fa-check"></i><b>4.4</b> Multidimensional Scaling</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="sec-mds.html"><a href="sec-mds.html#key-features-of-mds"><i class="fa fa-check"></i><b>4.4.1</b> Key features of MDS</a></li>
<li class="chapter" data-level="4.4.2" data-path="sec-mds.html"><a href="sec-mds.html#classical-scaling"><i class="fa fa-check"></i><b>4.4.2</b> Classical Scaling</a></li>
<li class="chapter" data-level="4.4.3" data-path="sec-mds.html"><a href="sec-mds.html#metric-mds"><i class="fa fa-check"></i><b>4.4.3</b> Metric MDS</a></li>
<li class="chapter" data-level="4.4.4" data-path="sec-mds.html"><a href="sec-mds.html#nonmetric-mds"><i class="fa fa-check"></i><b>4.4.4</b> Nonmetric MDS</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="exercises-2.html"><a href="exercises-2.html"><i class="fa fa-check"></i><b>4.5</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="kernels-and-nonlinearity.html"><a href="kernels-and-nonlinearity.html"><i class="fa fa-check"></i><b>5</b> Kernels and Nonlinearity</a>
<ul>
<li class="chapter" data-level="5.1" data-path="kernel-pca.html"><a href="kernel-pca.html"><i class="fa fa-check"></i><b>5.1</b> Kernel PCA</a></li>
<li class="chapter" data-level="5.2" data-path="exercises-3.html"><a href="exercises-3.html"><i class="fa fa-check"></i><b>5.2</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ch-nonlinear.html"><a href="ch-nonlinear.html"><i class="fa fa-check"></i><b>6</b> Manifold Learning</a>
<ul>
<li class="chapter" data-level="6.1" data-path="data-on-a-manifold.html"><a href="data-on-a-manifold.html"><i class="fa fa-check"></i><b>6.1</b> Data on a manifold</a></li>
<li class="chapter" data-level="6.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html"><i class="fa fa-check"></i><b>6.2</b> Isometric Feature Map (ISOMAP)</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#introduction"><i class="fa fa-check"></i><b>6.2.1</b> Introduction</a></li>
<li class="chapter" data-level="6.2.2" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#key-definitions"><i class="fa fa-check"></i><b>6.2.2</b> Key Definitions</a></li>
<li class="chapter" data-level="6.2.3" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#algorithm"><i class="fa fa-check"></i><b>6.2.3</b> Algorithm</a></li>
<li class="chapter" data-level="6.2.4" data-path="isometric-feature-map-isomap.html"><a href="isometric-feature-map-isomap.html#limitations-of-isomap"><i class="fa fa-check"></i><b>6.2.4</b> Limitations of ISOMAP</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html"><i class="fa fa-check"></i><b>6.3</b> Locally Linear Embeddings (LLEs)</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#introduction-1"><i class="fa fa-check"></i><b>6.3.1</b> Introduction</a></li>
<li class="chapter" data-level="6.3.2" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#algorithm-1"><i class="fa fa-check"></i><b>6.3.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.3.3" data-path="locally-linear-embeddings-lles.html"><a href="locally-linear-embeddings-lles.html#strengths-and-weaknesses-of-lle"><i class="fa fa-check"></i><b>6.3.3</b> Strengths and Weaknesses of LLE</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="laplacian-eigenmap.html"><a href="laplacian-eigenmap.html"><i class="fa fa-check"></i><b>6.4</b> Laplacian Eigenmap</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="laplacian-eigenmap.html"><a href="laplacian-eigenmap.html#algorithm-2"><i class="fa fa-check"></i><b>6.4.1</b> Algorithm</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html"><i class="fa fa-check"></i><b>6.5</b> Autoencoders (AEs)</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#introduction-2"><i class="fa fa-check"></i><b>6.5.1</b> Introduction</a></li>
<li class="chapter" data-level="6.5.2" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#algorithm-3"><i class="fa fa-check"></i><b>6.5.2</b> Algorithm</a></li>
<li class="chapter" data-level="6.5.3" data-path="autoencoders-aes.html"><a href="autoencoders-aes.html#example"><i class="fa fa-check"></i><b>6.5.3</b> Example</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="additional-methods.html"><a href="additional-methods.html"><i class="fa fa-check"></i><b>6.6</b> Additional methods</a></li>
<li class="chapter" data-level="6.7" data-path="exercises-4.html"><a href="exercises-4.html"><i class="fa fa-check"></i><b>6.7</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="ch-clustering.html"><a href="ch-clustering.html"><i class="fa fa-check"></i><b>7</b> Clustering</a>
<ul>
<li class="chapter" data-level="7.1" data-path="center-based.html"><a href="center-based.html"><i class="fa fa-check"></i><b>7.1</b> Center-based</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="center-based.html"><a href="center-based.html#k-means"><i class="fa fa-check"></i><b>7.1.1</b> k-means</a></li>
<li class="chapter" data-level="7.1.2" data-path="center-based.html"><a href="center-based.html#k-mediods"><i class="fa fa-check"></i><b>7.1.2</b> k-mediods</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="hierarchical.html"><a href="hierarchical.html"><i class="fa fa-check"></i><b>7.2</b> Hierarchical</a></li>
<li class="chapter" data-level="7.3" data-path="model-based.html"><a href="model-based.html"><i class="fa fa-check"></i><b>7.3</b> Model-based</a></li>
<li class="chapter" data-level="7.4" data-path="spectral.html"><a href="spectral.html"><i class="fa fa-check"></i><b>7.4</b> Spectral</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Unsupervised Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="nonnegative-matrix-factorization" class="section level2 hasAnchor" number="4.3">
<h2><span class="header-section-number">4.3</span> Nonnegative Matrix Factorization<a href="nonnegative-matrix-factorization.html#nonnegative-matrix-factorization" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In both PCA and SVD, we learn data-drive orthonormal feature vectors which we can use to decompose our data in an orderly fashion. Nonnegative matrix factorization (NMF) is again focused on learning a set latent vectors which can be used to approximate our data. However, we will add a few restrictions motivated by experimental data and a desire to increase interpretability of the results which will drastically alter the results.</p>
<p>For NMF, we focus on cases where <span class="math inline">\({\bf X}\)</span> is an <span class="math inline">\(N\times d\)</span> data matrix with the added condition that its entries are nonnegative. Notationally, we write <span class="math inline">\({\bf X}\in\mathbb{R}_{\ge 0}^{N\times d}\)</span> to indicate it is composed of nonnegative real values. The nonnegativity condition is a natural constraint for many experimental data sets, but we are also going to impose a similar constraint on our feature vectors and coefficients. More specifically, for a specific rank <span class="math inline">\(k\)</span>, we seek a coefficient matrix <span class="math inline">\({\bf W}\in\mathbb{R}_{\ge 0}^{N\times k}\)</span> and feature matrix <span class="math inline">\({\bf H}\in \mathbb{R}^{k\times d}_{\ge 0}\)</span> such that <span class="math inline">\({\bf WH}\)</span> is as close to <span class="math inline">\({\bf X}\)</span> as possible. The nonnegativity constraint on the elements of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> implies that <span class="math inline">\({\bf WH}\in\mathbb{R}_{\ge 0}^{N\times d}\)</span>. There are many different measures of proximity that one may use in NMF which are greater than zero and equal to zero if and only if <span class="math inline">\({\bf X}={\bf WH}\)</span>. The most common measures are</p>
<ul>
<li><p>Frobenius norm: <span class="math inline">\(\|{\bf X}-\hat{\bf X}\|_F.\)</span></p></li>
<li><p>Divergence: <span class="math inline">\(D({\bf X} \| \hat{\bf X}) = \sum_{i=1}^N\sum_{j=1}^d \left[{\bf X}_{ij} \log \frac{{\bf X}_{ij}}{\hat{\bf X}_{ij}} + \hat{\bf X}_{ij} - {\bf X}_{ij}\right]\)</span></p></li>
<li><p>Itakura-Saito Divergence: <span class="math display">\[D_{IS}({\bf X}, {\bf WH}) = \sum_{i=1}^N\sum_{j=1}^d \left[\frac{{\bf X}_{ij}}{({\bf WH})_{ij}} - \log\frac{{\bf X}_{ij}}{({\bf WH})_{ij}} -1  \right]\]</span></p></li>
</ul>
<p>These loss functions emphasize different features of the data and are often coupled with additional penalties or assumptions on the elements of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> which we will discuss at the end of the section. For now, let us focus on the primary motivation of NMF, which is to create more interpretable feature vectors (the rows of <span class="math inline">\({\bf H}\)</span>).</p>
<div id="interpretability-superpositions-and-positive-spans" class="section level3 hasAnchor" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> Interpretability, Superpositions, and Positive Spans<a href="nonnegative-matrix-factorization.html#interpretability-superpositions-and-positive-spans" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Consider the case of SVD where we can approximate a given vector <span class="math inline">\(\vec{x}_i\)</span> using the first <span class="math inline">\(k\)</span> right singular vectors as
<span class="math display">\[\vec{x}_i \approx \sum_{j=1}^k \sigma_j {\bf U}_{ij} \vec{v}_j.\]</span> Let us restrict ourselves to the case of nonnegative entries. Suppose for the moment that the <span class="math inline">\(\ell\)</span>th coordinate of <span class="math inline">\(\vec{x}_i\)</span> is (very close to) zero and this is reflected by our approximation as well so that <span class="math display">\[(\vec{x}_i)_\ell \approx \sum_{j=1}^k \sigma_j {\bf U}_{ij} (\vec{v}_j)_\ell \approx 0.\]</span>
The freedom of the coefficients <span class="math inline">\(\sigma_j {\bf U}_{ij}\)</span> and the features <span class="math inline">\((\vec{v}_j)_\ell\)</span> for <span class="math inline">\(j=1,\dots,k\)</span> to take any value in <span class="math inline">\(\mathbb{R}\)</span> prevents us from concluding that there is a comparable <code>near-zeroness</code> in the features or coefficients. In could be that case that <span class="math inline">\({\bf U}_{ij} (\vec{v}_j)_\ell\)</span> is near zero for all <span class="math inline">\(j=1,\dots,k\)</span> or that some subset are large and positive but are canceled out by a different subset that is large and negative. If, however, we restrict the coefficients and features to be zero this cancellation effect cannot occur. Features can only then be added but never subtracted. Under this restriction, <span class="math inline">\(\sum_{j=1}^k \sigma_j {\bf U}_{ij} (\vec{v}_j)_\ell\)</span> will only be close to zero if the coefficients are (near) zero for any feature vector which has a (large) positive entry in its <span class="math inline">\(\ell\)</span>th coordinate.</p>
<p>Thus, in a factorization of the form <span class="math display">\[\vec{x}_i \approx \sum_{j=1}^k \underbrace{{\bf W}_{ij}}_{\ge 0} \underbrace{\vec{h}_j}_{\in \mathbb{R}^d_{\ge}},\]</span>
we can only superimpose (add) features to approximate our data, we might expect the features themselves to look more like our data. In matrix notation, we have <span class="math display">\[{\bf X} = {\bf WH}\]</span> where the coefficients for each sample are stored in the rows of <span class="math inline">\({\bf W}\in\mathbb{R}^{N\times k}_{\ge 0}\)</span> and the features are transposed and listed as the rows of <span class="math inline">\({\bf H}\in\mathbb{R}^{k\times d}\)</span>.</p>
</div>
<div id="geometric-interpretation" class="section level3 hasAnchor" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> Geometric Interpretation<a href="nonnegative-matrix-factorization.html#geometric-interpretation" class="anchor-section" aria-label="Anchor link to header"></a></h3>
As a motivating example consider the case, where we have data <span class="math inline">\(\vec{x}_1,\dots,\vec{x}_N\in\mathbb{R}^3_{\ge 0}\)</span> for which there is a exact decomposition <span class="math display">\[{\bf X} = {\bf WH}\]</span> which is to say there are nonnegative coefficients, <span class="math inline">\({\bf W}\)</span> and feature vectors <span class="math inline">\(\vec{h}_1,\vec{h}_2 \in \mathbb{R}_{\ge 0}^3\)</span> such that <span class="math display">\[\vec{x}_i = {\bf W}_{i1}\vec{h}_1 + {\bf W}_{i2}\vec{h}_2 \qquad \text{ for } i =1,\dots,N.\]</span> The nonnegativity condition on data implies that <span class="math inline">\(\vec{x}_1,\dots,\vec{x}_N\)</span> reside in the positive orthant of <span class="math inline">\(\mathbb{R}^3.\)</span> The exact decomposition assumptions implies <span class="math inline">\(\{\vec{x}_1,\dots,\vec{x}_N\} \in \text{span}\{\vec{h}_1,\vec{h}_2\}\)</span> and furthermore the following more restricted picture holds.
<div class="figure"><span style="display:block;" id="fig:nmf-ex"></span>
<div class="plotly html-widget html-fill-item" id="htmlwidget-2b8cd778f99f38969314" style="width:672px;height:480px;"></div>
<script type="application/json" data-for="htmlwidget-2b8cd778f99f38969314">{"x":{"visdat":{"164682c13b2fc":["function () ","plotlyVisDat"],"16468414bd4ff":["function () ","data"]},"cur_data":"16468414bd4ff","attrs":{"164682c13b2fc":{"x":{},"y":{},"z":{},"name":"1st Feature","color":["red"],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","inherit":true},"16468414bd4ff":{"x":{},"y":{},"z":{},"name":"2nd Feature","color":["black"],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"lines","inherit":true},"16468414bd4ff.1":{"x":[2.2458658555579851,1.4189738247191617,1.7947618282360724,5.1982379050267378,2.0566012901386603,3.4791374787258711,4.7682105035254327,2.4287171183528122,2.3169614643758334,2.8031041612770111,2.8896561609509739,3.3078936796914191,6.0042957908457781,2.6174916620155293,2.8501389193892197,1.9878637902679244,2.209855144722614,3.2323477110359473,2.6457632141130119,3.2647929230376604,6.0868278327631096,2.3163721194278879,3.3554910312848922,2.0414525752754398,2.3184927014872896,3.3620000824413538,2.9826122254189773,2.4500607875818159,2.2536290982377833,4.8432302398622076,2.556564118826135,2.9351195189903487,2.8259701876054271,1.9928345766282269,3.226198665017169,4.3457550048270663,1.8874079817346661,3.3674540463546871,4.7811618165985674,2.3919384643031676,5.1149217605193016,2.7286357707043112,2.6416571280440806,2.8276991068579114,3.9096775508128356,2.2356580430068385,2.4510207922367289,2.2030990062782805,2.0686249659911278,2.3653257194826329,2.6736741831048136,3.5816308252113691,2.8728979403216663,2.2834706953007227,3.2488135373129774,3.1938196967537182,4.6278863241248658,1.5777737310654911,4.7431120993583962,2.5515663208972388,3.2733076957809915,4.4844019382456661,3.4076455417369655,2.2762438261257341,1.7091605664102774,3.5942052787671326,3.4489445298459449,3.4992788618351423,3.1294236848314636,3.8270724430309073,2.115477885524919,3.5097185310568952,3.5600896518094203,2.1729300337457063,2.3059397666658361,3.3339710028993137,1.7217715884266793,3.4773244780329828,2.1607511096682517,2.750088807455076,2.4409609964724255,1.4376713378332862,3.125053528773913,3.0297892559815871,1.7720172456196344,3.0986103303892985,2.5937526314762063,2.2223940256122283,2.2213359093531428,2.9489060334279436,1.3643523769044581,3.1727917602834244,3.9608338087092552,3.846296978294089,4.9141624349195068,1.528850006429578,1.7773322354484369,4.1730075031346008,2.7336318850033066,4.998603379210814],"y":[0.63628817004072125,0.42799359596745135,0.63947459313620991,1.3139508156111221,0.59695020016412548,1.279799422656493,1.5297585341918238,0.68405261584872756,0.61873122995686369,0.94612536732605712,0.82580857374928485,1.0608676075967431,1.887917359886488,0.82298695308754199,0.93808998029175683,0.81393995970092015,0.6907709070934781,1.2541419979359549,0.96551717998057929,0.968142145627369,2.0273843712571775,0.93093471090685609,0.97236214530611542,0.78395799778968056,0.67186840030474193,1.2693386363788366,1.2124575002159594,0.84961041015816685,0.78840979771715791,1.5045101601595512,0.85163194196736081,0.83575771406306854,0.80171271750986328,0.60432952851168353,1.2404719294185838,1.6058092424306036,0.65550377934017934,1.1838712300811234,1.3135146024592546,0.74733031147244211,1.4802641827293113,0.73254824344741054,0.69046555632450046,1.2210752020842197,1.3964366974858395,0.65155979331105263,0.87255373355690935,0.78763601508423309,0.77233386503503132,0.83487318673701116,0.69887397391785389,1.0858075630344552,1.1243730421488127,0.79926162336617079,0.98719437485141537,0.84118080223935487,1.4883654195149305,0.59649263497436178,1.2954656223887977,0.8836527996291208,1.0407668273615698,1.097918764477493,1.243923680530032,0.92392397452992392,0.60023816348717607,0.87748208366361646,1.2279152192633178,1.0302302128241407,0.8834022065864352,1.2544474268832932,0.76298146577393122,0.98793164024875402,0.93413575529978099,0.77786747995663708,0.81990652771841266,0.87841188694441719,0.5672233470833058,1.1660651084482561,0.57997502937013823,0.72604178364651173,0.7607790910648764,0.51807225721344574,0.94517039082836329,0.91614880841531487,0.50999603953950134,0.91934353086045006,0.73624672871202668,0.66425178271776286,0.75517969102393723,0.9285655090054028,0.48589105232701407,1.2674174104105653,1.3981892778153291,0.94491368837028911,1.6725810689018819,0.57727416163558631,0.51355573526065357,1.2067302614701585,0.90379545341875067,1.6265805965982196],"z":[1.4411318117463219,0.95374564495472003,1.3695880204498823,3.0714478961008749,1.3434468184385124,2.7217864184306171,3.3572190130707948,1.551742901914156,1.4240344680032049,2.0515129130422873,1.8660935016524942,2.3283923648154716,4.1636302217433139,1.8150365687561574,2.0461572451258969,1.6927295718263857,1.5256369131974266,2.6370615537251156,2.0569710638946477,2.1667887614509311,4.4097118264754673,1.943327167641669,2.1892595827173289,1.6519792496640113,1.512696245696068,2.6844293071196237,2.5251716677623488,1.8308067441394638,1.695529353606688,3.3279943881591549,1.8523125082601555,1.8903898636747316,1.8151579542147656,1.3448607816832154,2.6132720493029691,3.4117751335947752,1.412036881328709,2.5431716878550605,2.9996739593805097,1.6507487992174104,3.3339401069225989,1.6835126531021005,1.5986856895031381,2.5131191917682769,2.9891702457795049,1.4647993566634403,1.8691396286799247,1.6856343412258226,1.6372696913861431,1.7918548802435754,1.6181282930848802,2.4165140636621327,2.3599415009419262,1.718666314165622,2.1957626821218112,1.9434810674563143,3.2644519214914887,1.2611180966111597,2.9631653103364246,1.9047322077328606,2.2890603075769724,2.5904482978979799,2.6499236900735124,1.9248280611836905,1.289730829152546,2.0720782386124048,2.6303266460356172,2.3100250338740391,2.0027523786819987,2.7388800719779138,1.6296920180045171,2.2414339671207468,2.1605178104583214,1.6642438143928318,1.7568394996829988,2.0292946055510286,1.2369544440786657,2.5322650249184271,1.3329480585926445,1.6763427594453104,1.6814735882900713,1.1067888810958264,2.1047677329940293,2.0402582557052478,1.150308275166261,2.0572966710789231,1.66668846357593,1.4836547090851109,1.6347454441095914,2.0471368054591026,1.0407612890200062,2.649001754110671,3.0008002880484432,2.2272033277030232,3.6196864642958868,1.2208114663670706,1.1571357146665244,2.7184255334522893,1.9692568351306197,3.5575427796217571],"name":"Data","color":["blue"],"alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"scatter3d","mode":"markers","size":0.10000000000000001,"inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"camera":{"eye":{"x":-1.5,"y":-1.5,"z":1}},"xaxis":{"title":"x"},"yaxis":{"title":"y"},"zaxis":{"title":"z"}},"hovermode":"closest","showlegend":true},"source":"A","config":{"modeBarButtonsToAdd":["hoverclosest","hovercompare"],"showSendToCloud":false},"data":[{"x":[0,1.5312480579208523],"y":[0,0.31495088346374661],"z":[0,0.78480940089468243],"name":"1st Feature","type":"scatter3d","mode":"lines","marker":{"color":"rgba(255,0,0,1)","line":{"color":"rgba(255,0,0,1)"}},"textfont":{"color":"rgba(255,0,0,1)"},"error_y":{"color":"rgba(255,0,0,1)"},"error_x":{"color":"rgba(255,0,0,1)"},"line":{"color":"rgba(255,0,0,1)"},"frame":null},{"x":[0,0.71630781511966224],"y":[0,0.3991165974471888],"z":[0,0.78600679992932787],"name":"2nd Feature","type":"scatter3d","mode":"lines","marker":{"color":"rgba(0,0,0,1)","line":{"color":"rgba(0,0,0,1)"}},"textfont":{"color":"rgba(0,0,0,1)"},"error_y":{"color":"rgba(0,0,0,1)"},"error_x":{"color":"rgba(0,0,0,1)"},"line":{"color":"rgba(0,0,0,1)"},"frame":null},{"x":[2.2458658555579851,1.4189738247191617,1.7947618282360724,5.1982379050267378,2.0566012901386603,3.4791374787258711,4.7682105035254327,2.4287171183528122,2.3169614643758334,2.8031041612770111,2.8896561609509739,3.3078936796914191,6.0042957908457781,2.6174916620155293,2.8501389193892197,1.9878637902679244,2.209855144722614,3.2323477110359473,2.6457632141130119,3.2647929230376604,6.0868278327631096,2.3163721194278879,3.3554910312848922,2.0414525752754398,2.3184927014872896,3.3620000824413538,2.9826122254189773,2.4500607875818159,2.2536290982377833,4.8432302398622076,2.556564118826135,2.9351195189903487,2.8259701876054271,1.9928345766282269,3.226198665017169,4.3457550048270663,1.8874079817346661,3.3674540463546871,4.7811618165985674,2.3919384643031676,5.1149217605193016,2.7286357707043112,2.6416571280440806,2.8276991068579114,3.9096775508128356,2.2356580430068385,2.4510207922367289,2.2030990062782805,2.0686249659911278,2.3653257194826329,2.6736741831048136,3.5816308252113691,2.8728979403216663,2.2834706953007227,3.2488135373129774,3.1938196967537182,4.6278863241248658,1.5777737310654911,4.7431120993583962,2.5515663208972388,3.2733076957809915,4.4844019382456661,3.4076455417369655,2.2762438261257341,1.7091605664102774,3.5942052787671326,3.4489445298459449,3.4992788618351423,3.1294236848314636,3.8270724430309073,2.115477885524919,3.5097185310568952,3.5600896518094203,2.1729300337457063,2.3059397666658361,3.3339710028993137,1.7217715884266793,3.4773244780329828,2.1607511096682517,2.750088807455076,2.4409609964724255,1.4376713378332862,3.125053528773913,3.0297892559815871,1.7720172456196344,3.0986103303892985,2.5937526314762063,2.2223940256122283,2.2213359093531428,2.9489060334279436,1.3643523769044581,3.1727917602834244,3.9608338087092552,3.846296978294089,4.9141624349195068,1.528850006429578,1.7773322354484369,4.1730075031346008,2.7336318850033066,4.998603379210814],"y":[0.63628817004072125,0.42799359596745135,0.63947459313620991,1.3139508156111221,0.59695020016412548,1.279799422656493,1.5297585341918238,0.68405261584872756,0.61873122995686369,0.94612536732605712,0.82580857374928485,1.0608676075967431,1.887917359886488,0.82298695308754199,0.93808998029175683,0.81393995970092015,0.6907709070934781,1.2541419979359549,0.96551717998057929,0.968142145627369,2.0273843712571775,0.93093471090685609,0.97236214530611542,0.78395799778968056,0.67186840030474193,1.2693386363788366,1.2124575002159594,0.84961041015816685,0.78840979771715791,1.5045101601595512,0.85163194196736081,0.83575771406306854,0.80171271750986328,0.60432952851168353,1.2404719294185838,1.6058092424306036,0.65550377934017934,1.1838712300811234,1.3135146024592546,0.74733031147244211,1.4802641827293113,0.73254824344741054,0.69046555632450046,1.2210752020842197,1.3964366974858395,0.65155979331105263,0.87255373355690935,0.78763601508423309,0.77233386503503132,0.83487318673701116,0.69887397391785389,1.0858075630344552,1.1243730421488127,0.79926162336617079,0.98719437485141537,0.84118080223935487,1.4883654195149305,0.59649263497436178,1.2954656223887977,0.8836527996291208,1.0407668273615698,1.097918764477493,1.243923680530032,0.92392397452992392,0.60023816348717607,0.87748208366361646,1.2279152192633178,1.0302302128241407,0.8834022065864352,1.2544474268832932,0.76298146577393122,0.98793164024875402,0.93413575529978099,0.77786747995663708,0.81990652771841266,0.87841188694441719,0.5672233470833058,1.1660651084482561,0.57997502937013823,0.72604178364651173,0.7607790910648764,0.51807225721344574,0.94517039082836329,0.91614880841531487,0.50999603953950134,0.91934353086045006,0.73624672871202668,0.66425178271776286,0.75517969102393723,0.9285655090054028,0.48589105232701407,1.2674174104105653,1.3981892778153291,0.94491368837028911,1.6725810689018819,0.57727416163558631,0.51355573526065357,1.2067302614701585,0.90379545341875067,1.6265805965982196],"z":[1.4411318117463219,0.95374564495472003,1.3695880204498823,3.0714478961008749,1.3434468184385124,2.7217864184306171,3.3572190130707948,1.551742901914156,1.4240344680032049,2.0515129130422873,1.8660935016524942,2.3283923648154716,4.1636302217433139,1.8150365687561574,2.0461572451258969,1.6927295718263857,1.5256369131974266,2.6370615537251156,2.0569710638946477,2.1667887614509311,4.4097118264754673,1.943327167641669,2.1892595827173289,1.6519792496640113,1.512696245696068,2.6844293071196237,2.5251716677623488,1.8308067441394638,1.695529353606688,3.3279943881591549,1.8523125082601555,1.8903898636747316,1.8151579542147656,1.3448607816832154,2.6132720493029691,3.4117751335947752,1.412036881328709,2.5431716878550605,2.9996739593805097,1.6507487992174104,3.3339401069225989,1.6835126531021005,1.5986856895031381,2.5131191917682769,2.9891702457795049,1.4647993566634403,1.8691396286799247,1.6856343412258226,1.6372696913861431,1.7918548802435754,1.6181282930848802,2.4165140636621327,2.3599415009419262,1.718666314165622,2.1957626821218112,1.9434810674563143,3.2644519214914887,1.2611180966111597,2.9631653103364246,1.9047322077328606,2.2890603075769724,2.5904482978979799,2.6499236900735124,1.9248280611836905,1.289730829152546,2.0720782386124048,2.6303266460356172,2.3100250338740391,2.0027523786819987,2.7388800719779138,1.6296920180045171,2.2414339671207468,2.1605178104583214,1.6642438143928318,1.7568394996829988,2.0292946055510286,1.2369544440786657,2.5322650249184271,1.3329480585926445,1.6763427594453104,1.6814735882900713,1.1067888810958264,2.1047677329940293,2.0402582557052478,1.150308275166261,2.0572966710789231,1.66668846357593,1.4836547090851109,1.6347454441095914,2.0471368054591026,1.0407612890200062,2.649001754110671,3.0008002880484432,2.2272033277030232,3.6196864642958868,1.2208114663670706,1.1571357146665244,2.7184255334522893,1.9692568351306197,3.5575427796217571],"name":"Data","type":"scatter3d","mode":"markers","marker":{"color":"rgba(0,0,255,1)","size":[55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55,55],"sizemode":"area","line":{"color":"rgba(0,0,255,1)"}},"textfont":{"color":"rgba(0,0,255,1)","size":55},"error_y":{"color":"rgba(0,0,255,1)","width":55},"error_x":{"color":"rgba(0,0,255,1)","width":55},"line":{"color":"rgba(0,0,255,1)","width":55},"frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.20000000000000001,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p class="caption">
Figure 4.4: Data in the positive span of two vectors
</p>
</div>
<p>The data are constrained within the positive span of the vectors, a notion we may now define.</p>
<div class="definition">
<p><span id="def:pos-span" class="definition"><strong>Definition 4.2  (Positive Span) </strong></span>The positive span of a set of vectors <span class="math inline">\(\{\vec{h}_1,\dots,\vec{h}_k\}\in\mathbb{R}^d\)</span> is the set
<span class="math display">\[\Gamma\left(\{\vec{h}_1,\dots,\vec{h}_k\}\right) = \left\{\vec{v}\in\mathbb{R}^d \, \bigg|\, \vec{v} = \sum_{j=1}^k a_j \vec{h}_j, \, a_1,\dots,a_k \ge 0\right\}.\]</span>
This set is also called the simplicial cone or conical hull of <span class="math inline">\(\{\vec{h}_1,\dots,\vec{h}_k\}\)</span>.</p>
</div>
<p>In the motivating example above, our data live in the positive span of the two feature vectors, thus we say the data matrix <span class="math inline">\({\bf X}\)</span> has a nonnegative matrix factorization <span class="math inline">\({\bf WH}\)</span>.</p>
<p>Thus, we may view NMF as a restricted version of PCA or SVD where we have moved from spans to positive spans. This seemingly small change gives rise to some big problems. Even in this simple case above we have a uniqueness problem. Up to sign changes and ordering, the feature vectors in PCA and SVD were unique. However, we can find two alternative vectors <span class="math inline">\(\vec{h}_1&#39;\)</span> and <span class="math inline">\(\vec{h}_2&#39;\)</span> which still give a exact NMF. There are trivial cases. First, one can change ordering (<span class="math inline">\(\vec{h}_1&#39; =\vec{h}_2\)</span> and <span class="math inline">\(\vec{h}_2&#39; = \vec{h}_1\)</span>) which we avoid in PCA and SVD by assuming the corresponding singular values of PC variances are listed in decreasaing order. Secondly, we could rescale by setting <span class="math inline">\(\vec{h}_1&#39; = c\vec{h}_1\)</span> and rescaling the corresponding coefficients by a factor of <span class="math inline">\(1/c\)</span> for some <span class="math inline">\(c &gt; 0\)</span> which PCA and SVD avoid by fixing feature vectors to have unit length. The ordering issue is a labeling concern and may be ignored, whereas the rescaling issue can be addressed by adding constraints on the length of the feature vectors.</p>
<p>A third and far more subtle issue occurs because we do not enforce orthogonality. In @ref[fig:nmf-ex], imagine that the feature vectors are the arms of a folding fan. We could change the angle between our feature vectors by opening or closing the arms of the fan. So long as we do not close the fan too much (and leave our data outside the positive span) or open them too much (so that feature vectors have negative entries), we can still find a perfect reconstruction. This `folding fan’ issue can be addressed through additional penalties which we discuss in @ref{sec-nmf-ext}, but uniqueness cannot be eliminated entirely. Thus, we seek <strong>an</strong> NMF for our data rather than <strong>the</strong> NMF.</p>
</div>
<div id="finding-an-nmf-multiplicative-updates" class="section level3 hasAnchor" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> Finding an NMF: Multiplicative Updates<a href="nonnegative-matrix-factorization.html#finding-an-nmf-multiplicative-updates" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a given choice of error, the lack of a unique solution also means there is no closed form solution for <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span>. Thus, we will need to apply numerical optimization to find a <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> which minimizes the selected error, <span class="math display">\[\mathop{\mathrm{arg\,min}}_{{\bf W}\in\mathbb{R}^{N\times k}_{\ge 0}, {\bf H}\in\mathbb{R}^{k\times d}_{\ge 0}} D({\bf X},{\bf WH})\]</span>. The loss is a function of all of the entries of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span>. To apply any type of gradient based optimization, we need to compute the partial derivative of our loss with respect to each of the entries of these matrices.</p>
<p>As an example, we will focus on the divergence and show the relevant details. For gradient based optimization, we then need to compute <span class="math inline">\(\frac{\partial }{\partial {\bf W}_{ij}} D({\bf X}\| {\bf WH})\)</span> for <span class="math inline">\(`\le i \le N\)</span> and <span class="math inline">\(1\le j\le k\)</span> and <span class="math inline">\(\frac{\partial }{\partial {\bf H}_{j\ell}} D({\bf X}\| {\bf WH})\)</span> for <span class="math inline">\(1\le j \le k\)</span> and <span class="math inline">\(1\le \ell \le d.\)</span> Note that <span class="math display">\[({\bf WH})_{st} = \sum_{j=1}^k{\bf W}_{sk}{\bf H}_{kt}\]</span> so that
<span class="math display">\[\frac{\partial ({\bf WH})_{st}}{\partial ({\bf WH})_{ij}} = \begin{cases}
{\bf H}_{jt} &amp; s = i \\
0 &amp; s\ne i
\end{cases}\]</span>
Thus, we may may make use of the chain rule to conclude that
<span class="math display">\[\begin{align*}
\frac{\partial }{\partial {\bf W}_{ij}} D({bf X}\| {\bf WH}) &amp;= \frac{\partial }{\partial {\bf W}_{ij}}\sum_{st} \left({\bf X}_{st} \log(({\bf X})_{st}) - {\bf X}_{st} \log(({\bf WH})_{st}) + ({\bf WH})_st - ({\bf X})_{st}\right)\\
&amp;=\frac{\partial }{\partial {\bf W}_{ij}}\sum_{st} \left(- {\bf X}_{st} \log(({\bf WH})_{st}) + ({\bf WH})_{st} \right) \\
&amp;=\sum_{st}\left(\frac{{\bf X}_{st}}{({\bf WH}_{st})} + 1\right)\frac{\partial ({\bf WH})_{st}}{\partial ({\bf WH})_{ij}} \\
&amp;=\sum_{t=1}^d \left(-\frac{{\bf X}_{it}}{({\bf WH}_{it})} + 1\right){\bf H}_{jt}.
\end{align*}\]</span>
A similar calculation gives
<span class="math display">\[\frac{\partial }{\partial {\bf H}_{ij}} D({bf X}\| {\bf WH}) = \sum_{s=1}^N \left(-\frac{{\bf X}_{sj}}{({\bf WH})_{sj}} + 1\right) {\bf W}_{sj}.\]</span>
In a standard implementation of gradient descent, we choose a step size <span class="math inline">\(\epsilon &gt;0\)</span> and apply the updates
<span class="math display">\[\begin{equation}
\begin{split}
{\bf W}_{ij} \leftarrow {\bf W}_{ij} - \epsilon \sum_{t=1}^d \left(-\frac{{\bf X}_{it}}{({\bf WH}_{it})} + 1\right){\bf H}_{jt} \\
{\bf H}_{ij} \leftarrow {\bf H}_{ij} - \epsilon \sum_{s=1}^N \left(-\frac{{\bf X}_{sj}}{({\bf WH})_{sj}} + 1\right) {\bf W}_{sj}
\end{split}
\end{equation}\]</span>
to each entry of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> simultaneously. Alternatively, we could consider coordinate descent where we update each entry of <span class="math inline">\({\bf W}\)</span> (holding all other entries of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> constant) separately then do the same for <span class="math inline">\({\bf H}\)</span> then repeat. Each approach will converge to a local mode (though possibly different ones) when <span class="math inline">\(\epsilon\)</span> is sufficiently small. However, if <span class="math inline">\(\epsilon\)</span> is too small it may take many iterations to converge. Unfortunately, choosing <span class="math inline">\(\epsilon\)</span> can create a numerically unstable algorithm (that doesn’t converge at all) or one where we lose the nonnegativity condition on the entries of <span class="math inline">\({\bf W}\)</span> or <span class="math inline">\({\bf H}\)</span>.</p>
<p>To preserve nonnegativity, <span class="citation">[<a href="#ref-lee_seung">8</a>]</span> developed a set of state dependent step-sizes (one for each entry of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span>) which result in multiplicative rather than additive updates. IFor divergence, they take step sizes
<span class="math display">\[\epsilon_{ij}^{W} = \frac{{\bf W}_{ij}}{\sum_t {\bf H}_{jt}} \text{ and } \epsilon_{ij}^H = \frac{{\bf H}_{ij}}{\sum_s {\bf W}_{sj}}.\]</span>
These step size are proportional to the entries we are updating so that we take larger steps for larger entries of <span class="math inline">\({\bf W}\)</span> of <span class="math inline">\({\bf H}\)</span>. If we substitute these step sizes in the updates simplify to</p>
<p><span class="math display">\[\begin{equation}
\begin{split}
{\bf W}_{ij} \leftarrow {\bf W}_{ij} \left[\frac{\sum_t \left({\bf H}_{jt}{\bf X}_{it}/({\bf WH}_{it})\right)}{\sum_t {\bf H}_{jt}} \right] \\
{\bf H}_{ij} \leftarrow {\bf H}_{ij} \left[\frac{\sum_s\left({\bf W}_{si}{\bf X_{sj}}/({\bf WH})_{si}    \right)}{\sum_s {\bf W}_{si}}\right]
\end{split}
\end{equation}\]</span></p>
<p>indicating that we rescale the entries of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> by some nonnegative value which is guaranteed to preserve the nonnegativity of each entry. One can verify that the multiplicative updates simplify to one when <span class="math inline">\({\bf X}= {\bf WH}.\)</span> One can then iteratively update the entries of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> one at a time, in blocks, or all together until a local mode is reached.</p>
<p>Multiplicative update rules for other choices of loss are also available in <span class="citation">[<a href="#ref-lee_seung">8</a>]</span> but are also provided as exercises for the reader.</p>
</div>
<div id="nmf-in-practice" class="section level3 hasAnchor" number="4.3.4">
<h3><span class="header-section-number">4.3.4</span> NMF in practice<a href="nonnegative-matrix-factorization.html#nmf-in-practice" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a given choice of loss, standard NMF proceeds in the following manner.</p>
<ol style="list-style-type: decimal">
<li>Choose a rank <span class="math inline">\(k\)</span> and initial <span class="math inline">\({\bf W}_\in\mathbb{R}^{N\times k}_{\ge 0}\)</span> and <span class="math inline">\({\bf H}\in\mathbb{R}^{k\times d}_{\ge 0}\)</span>.</li>
<li>Apply the multiplicative rule until a local minimum is reached.</li>
<li>Repeats steps (1) and (2) for different initial conditions then select the final <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> which give the lowest overall loss.</li>
</ol>
<p>Different initial conditions (ICs) will converge to different local modes; reptition over multiple differents (ICs) will help find a better overall minimizer and avoid getting a final NMF which is trapped in a poor local mode. There is no guidance of how many ICs. Many packages default to five. More would be better but computational resources are not infinite, and finding even one mode though coordinate ascent can be slow.</p>
<p>The choice of <span class="math inline">\(k\)</span> is also challenging. In prior information or other project constraints dictate a specific choice of <span class="math inline">\(k\)</span> one should use that value. However, the preceding steps can be repeated over a range of <span class="math inline">\(k\)</span>. One can then compare plot the optimal loss as a function of <span class="math inline">\(k\)</span> and look for a cutoff where the error appears to saturate. Importantly, there are no connections between the rank <span class="math inline">\(k\)</span> NMF and rank <span class="math inline">\(k+1\)</span> NMF. Unlike, PCA or SVD, one cannot truncate the vectors and coefficients and attain an optimal solution for a lower-dimensional representation. Thus, separate fits at each choice of <span class="math inline">\(k\)</span> must be attained through separate runs of the numerical optimization. We show this approach applied to the first <span class="math inline">\(10^3\)</span> eights (to save computation time) from the MNIST dataset <span class="citation">[<a href="#ref-mnist">1</a>]</span> using Divergence loss with five runs per rank.</p>
<div class="example">
<p><span id="exm:mnist-nmf" class="example"><strong>Example 4.2  (NMF applied to MNIST) </strong></span></p>
<div class="sourceCode" id="cb12"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb12-1"><a href="nonnegative-matrix-factorization.html#cb12-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">185</span>)</span>
<span id="cb12-2"><a href="nonnegative-matrix-factorization.html#cb12-2" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;../../Datasets/digits.Rdata&quot;</span>)</span>
<span id="cb12-3"><a href="nonnegative-matrix-factorization.html#cb12-3" tabindex="-1"></a>eights <span class="ot">&lt;-</span> digits<span class="sc">$</span>pixels[digits<span class="sc">$</span>labels <span class="sc">==</span> <span class="dv">8</span>,][<span class="dv">1</span><span class="sc">:</span><span class="dv">1000</span>,]</span>
<span id="cb12-4"><a href="nonnegative-matrix-factorization.html#cb12-4" tabindex="-1"></a><span class="co"># mnist_nmf &lt;- nmf(eights, rank = 1:30, method = &quot;brunet&quot;, nrun = 5)</span></span>
<span id="cb12-5"><a href="nonnegative-matrix-factorization.html#cb12-5" tabindex="-1"></a><span class="co"># plot(mnist_nmf$measures$rss)</span></span></code></pre></div>
</div>
</div>
<div id="sec-nmf-ext" class="section level3 hasAnchor" number="4.3.5">
<h3><span class="header-section-number">4.3.5</span> Regularization and Interpretability<a href="nonnegative-matrix-factorization.html#sec-nmf-ext" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>For a random initialization of <span class="math inline">\({\bf W}\)</span> and <span class="math inline">\({\bf H}\)</span> the final features found by NMF may not look like our original data quite like our original data, which is contrary to our original goal of finding features which are more comparable to the data. Many extensions of NMF address this issue through the inclusion of additional penalties on the elements of <span class="math inline">\({\bf W}\)</span> or <span class="math inline">\({\bf H}\)</span> which induce sparsity and/or constrain the features to be more similar to the data. In practice, it is insufficient to apply penalties which depend on the scaling of <span class="math inline">\({\bf W}\)</span> or <span class="math inline">\({\bf H}\)</span> as these can typically be made arbitrarily small by increasing corresponding elements of <span class="math inline">\({\bf H}\)</span> or <span class="math inline">\({\bf W}\)</span> respectively. Thus, any penalty which depends on the scaling of one matrix often includes additional constraints on the other.</p>
<div id="volume-regularized-nmf" class="section level4 hasAnchor" number="4.3.5.1">
<h4><span class="header-section-number">4.3.5.1</span> Volume Regularized NMF<a href="nonnegative-matrix-factorization.html#volume-regularized-nmf" class="anchor-section" aria-label="Anchor link to header"></a></h4>
</div>
<div id="archetypes" class="section level4 hasAnchor" number="4.3.5.2">
<h4><span class="header-section-number">4.3.5.2</span> Archetypes<a href="nonnegative-matrix-factorization.html#archetypes" class="anchor-section" aria-label="Anchor link to header"></a></h4>
<!-- MDS -->
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body">
<div id="ref-mnist" class="csl-entry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline"><span class="smallcaps">Deng</span>, L. (2012). The mnist database of handwritten digit images for machine learning research. <em>IEEE Signal Processing Magazine</em> <strong>29</strong> 141–2.</div>
</div>
<div id="ref-lee_seung" class="csl-entry">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline"><span class="smallcaps">Lee</span>, D. and <span class="smallcaps">Seung</span>, H. S. (2000). <a href="https://proceedings.neurips.cc/paper_files/paper/2000/file/f9d1152547c0bde01830b7e8bd60024c-Paper.pdf">Algorithms for non-negative matrix factorization</a>. In <em>Advances in neural information processing systems</em> vol 13, (T. Leen, T. Dietterich and V. Tresp, ed). MIT Press.</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="sec-svd.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sec-mds.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/young1062/introUL03-linear_methods.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
